{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lGAuXecU_ZptjdKQShXFXtZFCgQOdeRi",
      "authorship_tag": "ABX9TyNfEe0mTlz73AMy9B1zDdM3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankit-lnmiit/deeplearning/blob/main/DL_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1\n",
        "\n",
        "## Answer 1: Linear Regression with l1-loss\n",
        "\n",
        "## a. l1-loss expression\n",
        "\n",
        "#### Training Examples: n\n",
        "#### Each Example has d features/dimension\n",
        "\n",
        "#### Let y be vector containing labels for each example\n",
        "$$\n",
        "\\hat y = \n",
        "\\begin{bmatrix}\n",
        "y_{1}  \\\\\n",
        "y_{2}  \\\\\n",
        "\\vdots \\\\\n",
        "y_{n} \n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Size of y: n x 1\n",
        "\n",
        "#### Let X be vector containing n training input x\n",
        "$$\n",
        "X =\n",
        "\\begin{bmatrix}\n",
        "x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
        "x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{n,1} & x_{n,2} & \\cdots & x_{n,d} \n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Size of X: n x d\n",
        "\n",
        "#### Let w be vector containing d weights\n",
        "$$\n",
        "w =\n",
        "\\begin{bmatrix}\n",
        "w_{1}\\\\\n",
        "w_{2}\\\\\n",
        "\\vdots\\\\\n",
        "w_{d} \n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Size of w: d x 1\n",
        "\n",
        "\n",
        "### l1-loss\n",
        "\n",
        "$$\n",
        "y = \\, < w, X > + \\, b \\quad\\quad b \\, is \\, bias\n",
        "$$\n",
        "\n",
        "$$\n",
        "Assume \\: b=0 \\: for \\: simplicity\n",
        "$$\n",
        "\n",
        "$$\n",
        "l(w,b) =\n",
        "\\Vert \\hat y - y \\Vert\n",
        "$$\n",
        "\n",
        "$$\n",
        "l(w,b) =\n",
        "\\Vert \\hat y - Xw\\Vert\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9sWK6I4KnTwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Optimal Linear Solution in Closed form\n",
        "\n",
        "$$\n",
        "l(w,b) =\n",
        "\\Vert \\hat y - Xw\\Vert\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "l(w,b) = \n",
        "\\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t\t\\hat y - Xw & \\mbox{if } y_{i} - x_{i}w_{i} \\geq 0 \\\\\n",
        "\t\tXw - \\hat y & \\mbox{if } y_{i} - x_{i}w_{i} < 0\n",
        "\t\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "#### From above definition, we can observe that l1-loss funcion is not differential in its domain, hence it is not possible to write closed form solution"
      ],
      "metadata": {
        "id": "Cy_YO0Qzyehg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. \n",
        "### First I wrote down the data(y,x) and model(y=wx+b).\n",
        "### Secondly, i wrote loss function i.e. measureness of goodness\n",
        "### Thirdly, I tried to optimize by differentiating butit was not differentiable\n"
      ],
      "metadata": {
        "id": "qTyw_x9Vla8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer 2: Expressivity of neural networks\n",
        "\n",
        "## a. \n",
        "\n",
        "<img src=\"https://github.com/ankit-lnmiit/deeplearning/blob/main/q2aaa.png?raw=true\" width=\"700\"/>\n",
        "\n",
        "### Hidden Layer 1\n",
        "$$\n",
        "z1 \\: = \\: w1 * X + \\: b1 \\\\\n",
        "y1 = σ(z1) \\\\\n",
        "$$\n",
        "\n",
        "$$\n",
        "z2 \\: = \\: w2 * X + \\: b2 \\\\\n",
        "y2 = σ(z2)\n",
        "$$\n",
        "\n",
        "### Output Layer\n",
        "$$\n",
        "z3 \\: = \\: w3 * y1 + \\: w3 * y2 +\\: b3 \\\\\n",
        "z3 \\: = \\: w3 * σ(\\: w1 * X + \\: b1) + \\: w4 * σ(\\: w2 * X + \\: b2) +\\: b3 \\\\\n",
        "y = z3\n",
        "$$\n",
        "\n",
        "$ Note: \\: σ(x) \\: is \\: step \\: function $\n",
        "\n",
        "$\n",
        "\\sigma(x) = \n",
        "\\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t\t1& \\mbox{if } x \\geq 0\\\\\n",
        "\t\t0&  \\mbox{if } x < 0\n",
        "\t\\end{array}\n",
        "\\right.\n",
        "$\n",
        "\n",
        "$$ \\\\ $$\n",
        "\n",
        "#### In order to realize the below function\n",
        "$$\n",
        "f(x) = \n",
        "\\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t\th& \\mbox{if } 0 < x < δ \\\\\n",
        "\t\t0&  otherwise\n",
        "\t\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "#### Lets start\n",
        "To realize the given function, we require two functions which we will we realize from each neuron.\n",
        "\n",
        "First one is y(x) = h for x > 0 \n",
        "and\n",
        "other is y(x) = -h for x > δ\n",
        "\n",
        "Then we can combine these two to get required function.\n",
        "\n",
        "Lets see first one, to handle x > 0 let keep b1 as 0 and w1 as 1\n",
        "\n",
        "Similarly to handle x > δ, lets keep w2 as 1 and b2 as -δ\n",
        "\n",
        "Now to add to get required function let keep w3 as h, w4 as -h and b3 as 0.\n",
        "\n",
        "Hence,\n",
        "\n",
        "w1 = 1\n",
        "w2 = 1\n",
        "w3 = h \n",
        "b1 = 0 \n",
        "b2 = -δ\n",
        "b3 = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q5GGIwd45cag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b.\n",
        "\n",
        "Now,\n",
        "\n",
        "$ x \\in [-B, B] $\n",
        "\n",
        "$ f $ is any abiratray, continous and bounded function.\n",
        "\n",
        "In above question, we have proved that we can realize box function using a hidden layer.\n",
        "\n",
        "Now, We can divide any continuos and bounded function f into boxes by dividing its input (x-axis) into small intervals just like we do in case of integration. By combining all these small intervals, we can approximate whole function. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XtIIsWTjoneW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c.\n",
        "\n",
        "Yes, it can be extended to d-dimension. The only difficulty in such network is computations of large numbers of values."
      ],
      "metadata": {
        "id": "6CszqiBLD7bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d. \n",
        "\n",
        "### First I analyzed thr inputs and outputs and guessed the model required for it. Wrote down the forward pass for the give model.\n",
        "### afterwarsds, analyzed the parameters.\n"
      ],
      "metadata": {
        "id": "xKd_Ju_GEf1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ASw4p5uzKY1U"
      }
    }
  ]
}